{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CNN1D Optimize.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelXJames/Signal-Denoising-Autoencoder/blob/master/CNN1D_Optimize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ontfmefnFpH9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Additional Packages to Install\n",
        "# !pip install pydrive\n",
        "# !pip install git+https://github.com/hyperopt/hyperopt.git\n",
        "# !pip install kopt\n",
        "# !pip install oauth2client\n",
        "# !pip install tensorboardcolab\n",
        "# !pip install pymongo\n",
        "# !pip install git+https://github.com/rthalley/dnspython.git\n",
        "# !pip install dnspython\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pydrive\n",
        "import pymongo\n",
        "import dns\n",
        "from tensorboardcolab import *\n",
        "from random import randint\n",
        "from keras.layers import Dense,Conv1D,Input\n",
        "from keras.layers import MaxPooling1D,UpSampling1D\n",
        "from keras.layers import Activation,BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.optimizers import RMSprop\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from kopt import CompileFN, KMongoTrials, test_fn\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "from hyperopt.mongoexp import MongoTrials\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#MongoTrials\n",
        "SRV_string  = ''\n",
        "client = pymongo.MongoClient(SRV_string)\n",
        "client.test_database\n",
        "print(client)\n",
        "hostname = ''\n",
        "port = 27017\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #Authenticate/Create PyDrive client \n",
        "# auth.authenticate_user()\n",
        "# gauth = GoogleAuth()\n",
        "# gauth.credentials = GoogleCredentials.get_application_default()\n",
        "# drive = GoogleDrive(gauth)\n",
        "\n",
        "#Altrnative Saving Method\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "  \n",
        "#tbc=TensorBoardColab()\n",
        "batch_size = 1000\n",
        "validation_split = 0.15\n",
        "timesteps = 2000\n",
        "features = 1\n",
        "global timesteps\n",
        "global fe#atures\n",
        "\n",
        "\n",
        "#Generate Test/Train Data\n",
        "def create_data(timesteps = 2000):\n",
        "  length = 1\n",
        "  dt = 0.0005\n",
        "  noise_factor = 0.35\n",
        "  data_1 = []\n",
        "  data_noisy_1 = []\n",
        "  for i in range(batch_size):\n",
        "      freq = 30*np.random.random() + 1\n",
        "      t = np.arange(0,length,dt)\n",
        "      #signal = np.arange(-0.5*length,0.5*length,dt)\n",
        "      signal = np.sin(2*np.pi*freq*t)\n",
        "      noisy_signal = signal+noise_factor*np.random.rand(len(t))\n",
        "      data_1.append(signal)\n",
        "      data_noisy_1.append(noisy_signal)\n",
        "  data_1 = np.array(data_1)\n",
        "  data_noisy_1 = np.array(data_noisy_1)\n",
        "  \n",
        "  #Scale Data - Need a better scaler than MinMaxScaler\n",
        "  scaler = MinMaxScaler(feature_range = (0,1))\n",
        "  data_1  = np.vstack((data_1,data_noisy_1))\n",
        "  data_1 = scaler.fit_transform(data_1)\n",
        "  signal = data_1[0:batch_size][:]\n",
        "  signal_noisy = data_1[batch_size:2*batch_size][:]\n",
        "  \n",
        "  #Input Shaping\n",
        "  features = 1\n",
        "  timesteps = len(signal[0])\n",
        "  shape = (batch_size,timesteps,features)\n",
        "  #Input shape should equal [samples, timesteps, features]\n",
        "  x = signal.reshape(shape)\n",
        "  y = signal_noisy.reshape(shape)\n",
        "  \n",
        "  split_point = int((1-validation_split)*batch_size)\n",
        "  \n",
        "  x_train = x[0:split_point,:,:]\n",
        "  y_train = y[0:split_point,:,:]\n",
        "  x_test = x[split_point:batch_size,:,:]\n",
        "  y_test = y[split_point:batch_size][:]\n",
        "\n",
        "  return (x_train,y_train,features),(x_test,y_test)\n",
        "\n",
        "def calculate_compressions_window_range(timesteps = 2000,window_size = 2,layers = 3,\n",
        "                                       filter_spacing = None):\n",
        "  window_range = []\n",
        "  for i in range(layers):\n",
        "    if timesteps%window_size != 0:\n",
        "      if timesteps%2 == 0:\n",
        "        tmp_window = 2\n",
        "      elif timesteps%3 == 0:\n",
        "        tmp_window = 3\n",
        "      elif timesteps%5 == 0:\n",
        "        tmp_window = 5\n",
        "      elif timesteps%7 == 0:\n",
        "        tmp_window = 7\n",
        "      window_range.append(tmp_window)\n",
        "      timesteps /= tmp_window\n",
        "    else:\n",
        "      window_range.append(window_size)\n",
        "      timesteps /= window_size\n",
        "    \n",
        "    if filter_spacing == 'half':\n",
        "      window_range = window_range[0:layers]\n",
        "    \n",
        "   \n",
        "  return window_range \n",
        "\n",
        "def create_filter_range(filter_spacing,max_filters,min_filters,layers):\n",
        "  #Linear Filter Range\n",
        "  if filter_spacing == 'lin':\n",
        "    filter_range = np.linspace(max_filters,min_filters,layers,\n",
        "                               dtype = 'int64')\n",
        "  #Half Filter Range\n",
        "  if filter_spacing == 'half':\n",
        "    if max_filters%2 != 0:\n",
        "      max_filters+=1\n",
        "    if min_filters%2 != 0:\n",
        "      min_filters+=1\n",
        "    filter_range = [max_filters]\n",
        "    x = max_filters\n",
        "    while x> min_filters:\n",
        "      x = x/2\n",
        "      filter_range.append(x)\n",
        "    if filter_range[-1] < min_filters:\n",
        "      del filter_range[-1]\n",
        "    filter_range = np.asarray(filter_range,dtype = 'int64')\n",
        "  \n",
        "  #Random Filter Range\n",
        "  if filter_spacing == 'random':\n",
        "    filter_range = [randint(min_filters, max_filters) for z in range(layers)]\n",
        "    filter_range.sort(reverse= True)\n",
        "    filter_range = np.asarray(filter_range)\n",
        "  \n",
        "  return filter_range\n",
        "  \n",
        "\n",
        "#Gnerate Model For Optimization\n",
        "def create_autoencoder(train_data,max_filters = 32, min_filters = 8, layers = 3,\n",
        "                      kernel_size = 3,window_size = 2,filter_spacing = 'lin'):\n",
        "  \n",
        "  timesteps = 2000\n",
        "  features = 1\n",
        "  \n",
        "  filter_range = create_filter_range(filter_spacing,max_filters,min_filters,\n",
        "                                    layers)\n",
        "  layers = len(filter_range)\n",
        "  \n",
        "  window_range = calculate_compressions_window_range(window_size = window_size,\n",
        "                                                     layers = layers)\n",
        "  \n",
        "  \n",
        "  inputs = Input(shape = (timesteps,features),name = 'Input')\n",
        "  encoded = inputs\n",
        "  \n",
        "  \n",
        "    \n",
        "  conv_name = ''\n",
        "  count = 1\n",
        "  for num_filters in filter_range: \n",
        "    conv_name = 'Conv_Layer_No._{0}'.format(count)\n",
        "    encoded = Conv1D(num_filters,kernel_size,padding = 'same',name = conv_name)(encoded)\n",
        "    count+=1\n",
        "    if(num_filters==max_filters):\n",
        "      encoded = BatchNormalization()(encoded)\n",
        "  \n",
        "    encoded = Activation('relu')(encoded)\n",
        "    encoded = MaxPooling1D(window_range[count-2],padding = 'same')(encoded)\n",
        "  \n",
        "  filter_range = np.sort(filter_range)\n",
        "  for num_filters in filter_range: \n",
        "    conv_name = 'Conv_Layer_No._{0}'.format(count)\n",
        "    encoded = Conv1D(num_filters,kernel_size,padding = 'same',name = conv_name)(encoded)\n",
        "    count+=1\n",
        "    encoded = Activation('relu')(encoded)\n",
        "    encoded = UpSampling1D(window_range[-(count-layers-1)])(encoded)\n",
        "    \n",
        "  decoded = Conv1D(1,kernel_size,activation = 'sigmoid',padding = 'same')(encoded)\n",
        "  \n",
        "  autoencoder = Model(inputs,decoded)\n",
        "  autoencoder.summary()\n",
        "  optimizer = RMSprop(lr=1e-3)\n",
        "  autoencoder.compile(optimizer = optimizer, loss = 'mse')\n",
        "  \n",
        "  return autoencoder\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#HyperParameter Range\n",
        "hyper_params = {\n",
        "    \"data\":{\n",
        "        'timesteps': 2000\n",
        "        \n",
        "        \n",
        "    },\n",
        "    \"model\":{\n",
        "        \"max_filters\":hp.choice('mx_filters',np.arange(32,320,dtype = 'int32')),\n",
        "        \"min_filters\":hp.choice('mn_filters',np.arange(4,32,dtype = 'int32')),\n",
        "        'layers':hp.choice('ly',np.arange(1,3,dtype = 'int32')),\n",
        "        'filter_spacing':hp.choice('fs',['lin','half'])\n",
        "        \n",
        "    },\n",
        "    \"fit\":{\n",
        "        'epochs': 1,\n",
        "        'patience': 3\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "#NN Evaluation\n",
        "db_name = 'Autoencoder'\n",
        "exp_name ='exp_1'\n",
        "objective = CompileFN(db_name,exp_name,\n",
        "                      data_fn = create_data,\n",
        "                      model_fn = create_autoencoder,\n",
        "                      loss_metric = 'loss',\n",
        "                      loss_metric_mode = 'min',\n",
        "                      valid_split = None,\n",
        "                      save_model = 'best',\n",
        "                      save_results = True)\n",
        "                      \n",
        "                      \n",
        "\n",
        "\n",
        "# model_file.Upload()\n",
        "# drive.CreateFile({'id': model_file.get('id')})\n",
        "\n",
        "#test_fn(objective, hyper_params)\n",
        "trials = Trials()\n",
        "best = fmin(objective, hyper_params, trials=trials, algo=tpe.suggest, max_evals=2)\n",
        "\n",
        "trial_data = trials.results\n",
        "print(trial_data)\n",
        "\n",
        "val_loss = []\n",
        "loss = []\n",
        "filter_spacing = []\n",
        "max_filters = []\n",
        "min_filters = []\n",
        "layers = []\n",
        "for i in range(len(trial_data)):\n",
        "    trial = trial_data[i]\n",
        "    val_loss.append('%.6f'%(trial['history']['loss']['val_loss'][0])) \n",
        "    loss.append('%.6f'%(trial['history']['loss']['loss'][0]))\n",
        "    filter_spacing.append(trial['param']['model']['filter_spacing'])\n",
        "    max_filters.append(trial['param']['model']['max_filters'])\n",
        "    min_filters.append(trial['param']['model']['min_filters'])\n",
        "    layers.append(trial['param']['model']['layers'])\n",
        "\n",
        "\n",
        "new_dict = {'val_loss':val_loss,'loss':loss,\n",
        "            'f_spacing':filter_spacing,'max_f':max_filters,'min_f':min_filters}\n",
        "\n",
        "data_frame = pd.DataFrame.from_dict(new_dict, orient='columns')\n",
        "print(data_frame)\n",
        "    \n",
        "\n",
        "#Mongo Trials - Cont'd\n",
        "#trials = MongoTrials(SRV_string,exp_key = 'Optimization_1')\n",
        "#trials = MongoTrials('mongo://{0}:{1}/{2}/jobs'.format(hostname,port,db_name), exp_key='exp1')\n",
        "#trials = KMongoTrials(db_name, exp_name,\n",
        "#                      ip=hostname,\n",
        "#                      port=port)\n",
        "\n",
        "\n",
        "#Save Results to Google Drive\n",
        "\n",
        "with open('/content/gdrive/My Drive/Optimization_Results.txt', 'w') as f:\n",
        "  f.write('{0}'.format(data_frame))\n",
        "!cat /content/gdrive/My\\ Drive/Optimization_Results.txt\n",
        "# model_file = drive.CreateFile({'title' : 'Optimization_Data.txt'})\n",
        "# model_file.SetContentFile(data_frame)\n",
        "# model_file.Upload()\n",
        "# drive.CreateFile({'id': model_file.get('id')})\n",
        "\n",
        "\n",
        " \n",
        "#Save Model to Google Drive\n",
        "# model_file = drive.CreateFile({'title' : 'CNN1D_v1.0.h5'})\n",
        "# model_file.SetContentFile('model.h5')\n",
        "# model_file.Upload()\n",
        "# drive.CreateFile({'id': model_file.get('id')})\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
