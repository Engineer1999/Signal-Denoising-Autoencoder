{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CNN1D Optimize(draft).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelXJames/Signal-Denoising-Autoencoder/blob/master/CNN1D_Optimize%20v0.3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ontfmefnFpH9",
        "colab_type": "code",
        "outputId": "cb39272a-87bf-403b-cba8-0e846594f65d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2536
        }
      },
      "cell_type": "code",
      "source": [
        "#Additional Packages to Install\n",
        "# !pip install pydrive\n",
        "# !pip install git+https://github.com/hyperopt/hyperopt.git\n",
        "# !pip install kopt\n",
        "# !pip install oauth2client\n",
        "# !pip install tensorboardcolab\n",
        "# !pip install pymongo\n",
        "# !pip install git+https://github.com/rthalley/dnspython.git\n",
        "# !pip install dnspython\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pydrive\n",
        "import pymongo\n",
        "import dns\n",
        "from tensorboardcolab import *\n",
        "from random import randint\n",
        "from keras.layers import Dense,Conv1D,Input\n",
        "from keras.layers import MaxPooling1D,UpSampling1D\n",
        "from keras.layers import Activation,BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.optimizers import RMSprop\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from kopt import CompileFN, KMongoTrials, test_fn\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "from hyperopt.mongoexp import MongoTrials\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#MongoTrials\n",
        "# SRV_string  = ''\n",
        "# client = pymongo.MongoClient(SRV_string)\n",
        "# client.test_database\n",
        "# print(client)\n",
        "# hostname = ''\n",
        "# port = 27017\n",
        "\n",
        "\n",
        "\n",
        "#Mounting Google Drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# #Authenticate/Create PyDrive client \n",
        "# auth.authenticate_user()\n",
        "# gauth = GoogleAuth()\n",
        "# gauth.credentials = GoogleCredentials.get_application_default()\n",
        "# drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "  \n",
        "#tbc=TensorBoardColab()\n",
        "batch_size = 1000\n",
        "validation_split = 0.15\n",
        "timesteps = 2000\n",
        "features = 1\n",
        "global timesteps\n",
        "global features\n",
        "\n",
        "\n",
        "#Generate Test/Train Data\n",
        "def create_data(timesteps = 2000):\n",
        "  length = 1\n",
        "  dt = 0.0005\n",
        "  noise_factor = 0.35\n",
        "  data_1 = []\n",
        "  data_noisy_1 = []\n",
        "  for i in range(batch_size):\n",
        "      freq = 30*np.random.random() + 1\n",
        "      t = np.arange(0,length,dt)\n",
        "      #signal = np.arange(-0.5*length,0.5*length,dt)\n",
        "      signal = np.sin(2*np.pi*freq*t)\n",
        "      noisy_signal = signal+noise_factor*np.random.rand(len(t))\n",
        "      data_1.append(signal)\n",
        "      data_noisy_1.append(noisy_signal)\n",
        "  data_1 = np.array(data_1)\n",
        "  data_noisy_1 = np.array(data_noisy_1)\n",
        "  \n",
        "  #Scale Data - Need a better scaler than MinMaxScaler\n",
        "  scaler = MinMaxScaler(feature_range = (0,1))\n",
        "  data_1  = np.vstack((data_1,data_noisy_1))\n",
        "  data_1 = scaler.fit_transform(data_1)\n",
        "  signal = data_1[0:batch_size][:]\n",
        "  signal_noisy = data_1[batch_size:2*batch_size][:]\n",
        "  \n",
        "  #Input Shaping\n",
        "  features = 1\n",
        "  timesteps = len(signal[0])\n",
        "  shape = (batch_size,timesteps,features)\n",
        "  #Input shape should equal [samples, timesteps, features]\n",
        "  x = signal.reshape(shape)\n",
        "  y = signal_noisy.reshape(shape)\n",
        "  \n",
        "  split_point = int((1-validation_split)*batch_size)\n",
        "  \n",
        "  x_train = x[0:split_point,:,:]\n",
        "  y_train = y[0:split_point,:,:]\n",
        "  x_test = x[split_point:batch_size,:,:]\n",
        "  y_test = y[split_point:batch_size][:]\n",
        "\n",
        "  return (x_train,y_train,features),(x_test,y_test)\n",
        "\n",
        "\n",
        "\n",
        "#Gnerate Model For Optimization\n",
        "def create_autoencoder(train_data,max_filters = 32, min_filters = 8, layers = 3,\n",
        "                      kernel_size = 3,filter_spacing = 'lin'):\n",
        "  \n",
        "  timesteps = 2000\n",
        "  features = 1\n",
        "  inputs = Input(shape = (timesteps,features),name = 'Input')\n",
        "  encoded = inputs\n",
        "  \n",
        "  #Linear Filter Range\n",
        "  if filter_spacing == 'lin':\n",
        "    filter_range = np.linspace(max_filters,min_filters,layers,\n",
        "                               dtype = 'int64')\n",
        "  #Half Filter Range\n",
        "  if filter_spacing == 'half':\n",
        "    if max_filters%2 != 0:\n",
        "      max_filters+=1\n",
        "    if min_filters%2 != 0:\n",
        "      min_filters+=1\n",
        "    filter_range = [max_filters]\n",
        "    x = max_filters\n",
        "    while x> min_filters:\n",
        "      x = x/2\n",
        "      filter_range.append(x)\n",
        "    if filter_range[-1] < min_filters:\n",
        "      del filter_range[-1]\n",
        "    filter_range = np.asarray(filter_range,dtype = 'int64')\n",
        "  \n",
        "  #Random Filter Range\n",
        "  if filter_spacing == 'random':\n",
        "    filter_range = [randint(min_filters, max_filters) for z in range(layers)]\n",
        "    filter_range.sort(reverse= True)\n",
        "    filter_range = np.asarray(filter_range)\n",
        "    \n",
        "  conv_name = ''\n",
        "  count = 1\n",
        "  for num_filters in filter_range: \n",
        "    conv_name = 'Conv_Layer_No._{0}'.format(count)\n",
        "    encoded = Conv1D(num_filters,kernel_size,padding = 'same',name = conv_name)(encoded)\n",
        "    count+=1\n",
        "    if(num_filters==max_filters):\n",
        "      encoded = BatchNormalization()(encoded)\n",
        "  \n",
        "    encoded = Activation('relu')(encoded)\n",
        "    encoded = MaxPooling1D(2,padding = 'same')(encoded)\n",
        "  \n",
        "  filter_range = np.sort(filter_range)\n",
        "  for num_filters in filter_range: \n",
        "    conv_name = 'Conv_Layer_No._{0}'.format(count)\n",
        "    encoded = Conv1D(num_filters,kernel_size,padding = 'same',name = conv_name)(encoded)\n",
        "    count+=1\n",
        "    encoded = Activation('relu')(encoded)\n",
        "    encoded = UpSampling1D(2)(encoded)\n",
        "    \n",
        "  decoded = Conv1D(1,kernel_size,activation = 'sigmoid',padding = 'same')(encoded)\n",
        "  \n",
        "  autoencoder = Model(inputs,decoded)\n",
        "  autoencoder.summary()\n",
        "  optimizer = RMSprop(lr=1e-3)\n",
        "  autoencoder.compile(optimizer = optimizer, loss = 'mse')\n",
        "  \n",
        "  return autoencoder\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#HyperParameter Range\n",
        "hyper_params = {\n",
        "    \"data\":{\n",
        "        'timesteps': 2000\n",
        "        \n",
        "        \n",
        "    },\n",
        "    \"model\":{\n",
        "        \"max_filters\":hp.choice('mx_filters',np.arange(32,320,dtype = 'int32')),\n",
        "        \"min_filters\":hp.choice('mn_filters',np.arange(4,32,dtype = 'int32')),\n",
        "        'layers':hp.choice('ly',np.arange(1,3,dtype = 'int32')),\n",
        "        'filter_spacing':hp.choice('fs',['lin','half','random'])\n",
        "        \n",
        "    },\n",
        "    \"fit\":{\n",
        "        'epochs': 1,\n",
        "        'patience': 3\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "#NN Evaluation\n",
        "db_name = 'Autoencoder'\n",
        "exp_name ='exp_1'\n",
        "objective = CompileFN(db_name,exp_name,\n",
        "                      data_fn = create_data,\n",
        "                      model_fn = create_autoencoder,\n",
        "                      loss_metric = 'loss',\n",
        "                      loss_metric_mode = 'min',\n",
        "                      valid_split = None,\n",
        "                      save_model = 'best',\n",
        "                      save_results = True)\n",
        "                      \n",
        "                      \n",
        "\n",
        "\n",
        "\n",
        "trials = Trials()\n",
        "best = fmin(objective, hyper_params, trials=trials, algo=tpe.suggest, max_evals=2)\n",
        "\n",
        "trial_data = trials.results\n",
        "\n",
        "#Organize trials into table\n",
        "val_loss = []\n",
        "loss = []\n",
        "filter_spacing = []\n",
        "max_filters = []\n",
        "min_filters = []\n",
        "layers = []\n",
        "for i in range(len(trial_data)):\n",
        "    trial = trial_data[i]\n",
        "    val_loss.append('%.6f'%(trial['history']['loss']['val_loss'][0])) \n",
        "    loss.append('%.6f'%(trial['history']['loss']['loss'][0]))\n",
        "    filter_spacing.append(trial['param']['model']['filter_spacing'])\n",
        "    max_filters.append(trial['param']['model']['max_filters'])\n",
        "    min_filters.append(trial['param']['model']['min_filters'])\n",
        "    layers.append(trial['param']['model']['layers'])\n",
        "\n",
        "\n",
        "new_dict = {'val_loss':val_loss,'loss':loss,\n",
        "            'f_spacing':filter_spacing,'max_f':max_filters,'min_f':min_filters}\n",
        "\n",
        "data_frame = pd.DataFrame.from_dict(new_dict, orient='columns')\n",
        "print(data_frame)\n",
        "    \n",
        "\n",
        "#Mongo Trials - Cont'd\n",
        "#trials = MongoTrials(SRV_string,exp_key = 'Optimization_1')\n",
        "#trials = MongoTrials('mongo://{0}:{1}/{2}/jobs'.format(hostname,port,db_name), exp_key='exp1')\n",
        "#trials = KMongoTrials(db_name, exp_name,\n",
        "#                      ip=hostname,\n",
        "#                      port=port)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "#Save Model to Google Drive\n",
        "# model_file = drive.CreateFile({'title' : 'CNN1D_v1.0.h5'})\n",
        "# model_file.SetContentFile('model.h5')\n",
        "# model_file.Upload()\n",
        "# drive.CreateFile({'id': model_file.get('id')})\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-12-30 05:37:42,783 [INFO] tpe_transform took 0.022910 seconds\n",
            "2018-12-30 05:37:42,785 [INFO] TPE using 0 trials\n",
            "2018-12-30 05:37:42,799 [INFO] Load data...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MongoClient(host=['autoencoderoptimization-shard-00-02-08kaz.mongodb.net:27017', 'autoencoderoptimization-shard-00-00-08kaz.mongodb.net:27017', 'autoencoderoptimization-shard-00-01-08kaz.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, authsource='admin', replicaset='AutoEncoderOptimization-shard-0', ssl=True, retrywrites=True)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2018-12-30 05:37:43,636 [INFO] Fit...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Input (InputLayer)           (None, 2000, 1)           0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._1 (Conv1D)    (None, 2000, 44)          176       \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 2000, 44)          176       \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 2000, 44)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_9 (MaxPooling1 (None, 1000, 44)          0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._2 (Conv1D)    (None, 1000, 22)          2926      \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 1000, 22)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_10 (MaxPooling (None, 500, 22)           0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._3 (Conv1D)    (None, 500, 11)           737       \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 500, 11)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_11 (MaxPooling (None, 250, 11)           0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._4 (Conv1D)    (None, 250, 5)            170       \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 250, 5)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_12 (MaxPooling (None, 125, 5)            0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._5 (Conv1D)    (None, 125, 5)            80        \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 125, 5)            0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_9 (UpSampling1 (None, 250, 5)            0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._6 (Conv1D)    (None, 250, 11)           176       \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 250, 11)           0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_10 (UpSampling (None, 500, 11)           0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._7 (Conv1D)    (None, 500, 22)           748       \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 500, 22)           0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_11 (UpSampling (None, 1000, 22)          0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._8 (Conv1D)    (None, 1000, 44)          2948      \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 1000, 44)          0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_12 (UpSampling (None, 2000, 44)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 2000, 1)           133       \n",
            "=================================================================\n",
            "Total params: 8,270\n",
            "Trainable params: 8,182\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "['loss']\n",
            "Train on 850 samples, validate on 150 samples\n",
            "Epoch 1/1\n",
            " - 9s - loss: 0.0420 - val_loss: 0.0162\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2018-12-30 05:38:11,890 [INFO] Evaluate...\n",
            "2018-12-30 05:38:15,225 [INFO] Done!\n",
            "2018-12-30 05:38:15,253 [INFO] tpe_transform took 0.026401 seconds\n",
            "2018-12-30 05:38:15,254 [INFO] TPE using 1/1 trials with best loss 0.016185\n",
            "2018-12-30 05:38:15,265 [INFO] Load data...\n",
            "2018-12-30 05:38:16,061 [INFO] Fit...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Input (InputLayer)           (None, 2000, 1)           0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._1 (Conv1D)    (None, 2000, 290)         1160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 2000, 290)         1160      \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 2000, 290)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_13 (MaxPooling (None, 1000, 290)         0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._2 (Conv1D)    (None, 1000, 145)         126295    \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 1000, 145)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_14 (MaxPooling (None, 500, 145)          0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._3 (Conv1D)    (None, 500, 72)           31392     \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 500, 72)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_15 (MaxPooling (None, 250, 72)           0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._4 (Conv1D)    (None, 250, 36)           7812      \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 250, 36)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_16 (MaxPooling (None, 125, 36)           0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._5 (Conv1D)    (None, 125, 36)           3924      \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 125, 36)           0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_13 (UpSampling (None, 250, 36)           0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._6 (Conv1D)    (None, 250, 72)           7848      \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 250, 72)           0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_14 (UpSampling (None, 500, 72)           0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._7 (Conv1D)    (None, 500, 145)          31465     \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 500, 145)          0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_15 (UpSampling (None, 1000, 145)         0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._8 (Conv1D)    (None, 1000, 290)         126440    \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 1000, 290)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_16 (UpSampling (None, 2000, 290)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 2000, 1)           871       \n",
            "=================================================================\n",
            "Total params: 338,367\n",
            "Trainable params: 337,787\n",
            "Non-trainable params: 580\n",
            "_________________________________________________________________\n",
            "['loss']\n",
            "Train on 850 samples, validate on 150 samples\n",
            "Epoch 1/1\n",
            " - 13s - loss: 0.0350 - val_loss: 0.0081\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2018-12-30 05:38:52,246 [INFO] Evaluate...\n",
            "2018-12-30 05:38:57,060 [INFO] Done!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[{'loss': 0.01618463173508644, 'status': 'ok', 'eval': {'loss': 0.01618463173508644}, 'param': {'data': {'timesteps': 2000}, 'fit': {'epochs': 1, 'patience': 3, 'batch_size': 32, 'early_stop_monitor': 'val_loss'}, 'model': {'filter_spacing': 'half', 'layers': 1, 'max_filters': 43, 'min_filters': 4}}, 'path': {'model': '/root/.kopt/data//Autoencoder/exp_1//train_models/c71a4995-ffda-442f-bc8c-a1561ba3e901.h5', 'results': '/root/.kopt/data//Autoencoder/exp_1//train_models/c71a4995-ffda-442f-bc8c-a1561ba3e901.json'}, 'name': {'data': 'create_data', 'model': 'create_autoencoder', 'optim_metric': 'loss', 'optim_metric_mode': 'loss'}, 'history': {'params': {'batch_size': 32, 'epochs': 1, 'steps': None, 'samples': 850, 'verbose': 2, 'do_validation': True, 'metrics': ['loss', 'val_loss']}, 'loss': {'epoch': [0], 'val_loss': [0.016184631971021494], 'loss': [0.04198666546274634]}}, 'time': {'start': '2018-12-30 05:37:42.798942', 'end': '2018-12-30 05:38:15.224955', 'duration': {'total': 32.426013, 'dataload': 0.25152, 'training': 32.174493}}}, {'loss': 0.008108320645987988, 'status': 'ok', 'eval': {'loss': 0.008108320645987988}, 'param': {'data': {'timesteps': 2000}, 'fit': {'epochs': 1, 'patience': 3, 'batch_size': 32, 'early_stop_monitor': 'val_loss'}, 'model': {'filter_spacing': 'half', 'layers': 1, 'max_filters': 290, 'min_filters': 25}}, 'path': {'model': '/root/.kopt/data//Autoencoder/exp_1//train_models/2f78e313-d03d-4ea2-af98-5d8818884475.h5', 'results': '/root/.kopt/data//Autoencoder/exp_1//train_models/2f78e313-d03d-4ea2-af98-5d8818884475.json'}, 'name': {'data': 'create_data', 'model': 'create_autoencoder', 'optim_metric': 'loss', 'optim_metric_mode': 'loss'}, 'history': {'params': {'batch_size': 32, 'epochs': 1, 'steps': None, 'samples': 850, 'verbose': 2, 'do_validation': True, 'metrics': ['loss', 'val_loss']}, 'loss': {'epoch': [0], 'val_loss': [0.00810831895718972], 'loss': [0.03497549022602684]}}, 'time': {'start': '2018-12-30 05:38:15.265511', 'end': '2018-12-30 05:38:57.060483', 'duration': {'total': 41.794972, 'dataload': 0.219787, 'training': 41.575185}}}]\n",
            "  f_spacing      loss  max_f  min_f  val_loss\n",
            "0      half  0.041987     43      4  0.016185\n",
            "1      half  0.034975    290     25  0.008108\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}