{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN1D Optimize(draft).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ontfmefnFpH9",
        "colab_type": "code",
        "outputId": "a10ab34d-3449-42b9-9d9c-6de09cba4b8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1683
        }
      },
      "cell_type": "code",
      "source": [
        "#Additional Packages to Install\n",
        "# !pip install pydrive\n",
        "# !pip install git+https://github.com/hyperopt/hyperopt.\n",
        "# !pip install kopt\n",
        "# !pip install oauth2client\n",
        "# !pip install tensorboardcolab\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pydrive\n",
        "from tensorboardcolab import *\n",
        "from random import randint\n",
        "from keras.layers import Dense,Conv1D,Input\n",
        "from keras.layers import MaxPooling1D,UpSampling1D\n",
        "from keras.layers import Activation,BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.optimizers import RMSprop\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from kopt import CompileFN, KMongoTrials, test_fn\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "\n",
        "\n",
        "#Authenticate/Create PyDrive client \n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#Create File in Collab\n",
        "#model_file = drive.CreateFile({'title' : 'backup.json'})\n",
        "#model_file.SetContentFile('model.h5')\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "#tbc=TensorBoardColab()\n",
        "\n",
        "batch_size = 1000\n",
        "validation_split = 0.15\n",
        "timesteps = 2000\n",
        "features = 1\n",
        "global timesteps\n",
        "global features\n",
        "\n",
        "def create_data():\n",
        "  length = 1\n",
        "  dt = 0.0005\n",
        "  noise_factor = 0.35\n",
        "  data = []\n",
        "  data_noisy = []\n",
        "  for i in range(batch_size):\n",
        "      freq = 30*np.random.random() + 1\n",
        "      t = np.arange(0,length,dt)\n",
        "      #signal = np.arange(-0.5*length,0.5*length,dt)\n",
        "      signal = np.sin(2*np.pi*freq*t)\n",
        "      noisy_signal = signal+noise_factor*np.random.rand(len(t))\n",
        "      data.append(signal)\n",
        "      data_noisy.append(noisy_signal)\n",
        "  data = np.array(data)\n",
        "  data_noisy = np.array(data_noisy)\n",
        "  \n",
        "  #Scale Data - Need a better scaler than MinMaxScaler\n",
        "  scaler = MinMaxScaler(feature_range = (0,1))\n",
        "  data  = np.vstack((signal,signal_noisy))\n",
        "  data = scaler.fit_transform(data)\n",
        "  signal = data[0:batch_size][:]\n",
        "  signal_noisy = data[batch_size:2*batch_size][:]\n",
        "  \n",
        "  #Input Shaping\n",
        "  features = 1\n",
        "  timesteps = len(signal[0])\n",
        "  shape = (batch_size,timesteps,features)\n",
        "  #Input shape should equal [samples, timesteps, features]\n",
        "  x = signal.reshape(shape)\n",
        "  y = signal_noisy.reshape(shape)\n",
        "  \n",
        "  x_train = x[0:(1-validation_split)*batch_size][:]\n",
        "  y_train = y[0:(1-validation_split)*batch_size][:]\n",
        "  x_test = x[(1-validation_split)*batch_size:batch_size][:]\n",
        "  y_test = y[(1-validation_split)*batch_size:batch_size][:]\n",
        "\n",
        "  return (x_train,y_train,features),(x_test,y_test)\n",
        "\n",
        "#Create Test/Train Data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #Prediction Data\n",
        "# x_pred = signal_noisy[0][:]\n",
        "# #Pre Autoencoder\n",
        "# x_pred_pre = signal_noisy[0] \n",
        "# x_pred_pre = x_pred_pre.reshape((-1,len(x_pred_pre)))\n",
        "# x_pred_pre = scaler.inverse_transform(x_pred_pre)\n",
        "# #Clean\n",
        "# clean_signal = signal[0][:]\n",
        "# clean_signal = clean_signal.reshape((-1,len(clean_signal)))\n",
        "# clean_signal = scaler.inverse_transform(clean_signal)\n",
        "\n",
        "\n",
        "\n",
        "#For Optimization\n",
        "def create_autoencoder(train_data,max_filters = 32, min_filters = 8, layers = 3,\n",
        "                      kernel_size = 3,filter_spacing = 'lin'):\n",
        "  \n",
        "  timesteps = 2000\n",
        "  features = 1\n",
        "  inputs = Input(shape = (timesteps,features))\n",
        "  encoded = inputs\n",
        "  \n",
        "  #Linear Filter Range\n",
        "  if filter_spacing == 'lin':\n",
        "    filter_range = np.linspace(max_filters,min_filters,layers,\n",
        "                               dtype = 'int64')\n",
        "  #Half Filter Range\n",
        "  if filter_spacing == 'half':\n",
        "    if max_filters%2 != 0:\n",
        "      max_filters+=1\n",
        "    if min_filters%2 != 0:\n",
        "      min_filters+=1\n",
        "    \n",
        "    \n",
        "    filter_range = [max_filters]\n",
        "    x = max_filters\n",
        "    while x> min_filters:\n",
        "      x = x/2\n",
        "      filter_range.append(x)\n",
        "    if filter_range[-1] < min_filters:\n",
        "      del filter_range[-1]\n",
        "    filter_range = np.asarray(filter_range,dtype = 'int64')\n",
        "  \n",
        "  #Random Filter Range\n",
        "  if filter_spacing == 'random':\n",
        "    filter_range = [randint(min_filters, max_filters) for z in range(layers)]\n",
        "    filter_range.sort(reverse= True)\n",
        "    filter_range = np.asarray(filter_range)\n",
        "    \n",
        "  \n",
        "  for num_filters in filter_range: \n",
        "    encoded = Conv1D(num_filters,kernel_size,padding = 'same')(encoded)\n",
        "  \n",
        "    if(num_filters==max_filters):\n",
        "      encoded = BatchNormalization()(encoded)\n",
        "  \n",
        "    encoded = Activation('relu')(encoded)\n",
        "    encoded = MaxPooling1D(2,padding = 'same')(encoded)\n",
        "  \n",
        "  filter_range = np.sort(filter_range)\n",
        "  for num_filters in filter_range: \n",
        "    encoded = Conv1D(num_filters,kernel_size,padding = 'same')(encoded)\n",
        "  \n",
        "    encoded = Activation('relu')(encoded)\n",
        "    encoded = UpSampling1D(2)(encoded)\n",
        "    \n",
        "  decoded = Conv1D(1,kernel_size,activation = 'sigmoid',padding = 'same')(encoded)\n",
        "  \n",
        "  autoencoder = Model(inputs,decoded)\n",
        "  \n",
        "  return autoencoder\n",
        "\n",
        "\n",
        "#NOT THE OPTIMIZED MODEL\n",
        "def create_Conv1D():\n",
        "  inputs = Input(shape = (timesteps,features))\n",
        "  \n",
        "  encoded = Conv1D(32,3, padding = 'same')(inputs)\n",
        "  encoded = BatchNormalization()(encoded)\n",
        "  encoded = Activation('relu')(encoded)\n",
        "  encoded = MaxPooling1D(2,padding = 'same')(encoded)\n",
        "  \n",
        "  \n",
        "  encoded = Conv1D(16,3,padding = 'same')(encoded)\n",
        "  encoded = Activation('relu')(encoded)\n",
        "  encoded = MaxPooling1D(2,padding = 'same')(encoded)\n",
        "  \n",
        "  encoded = Conv1D(8,3, padding = 'same')(encoded)\n",
        "  encoded = Activation('relu')(encoded)\n",
        "  encoded = MaxPooling1D(2,padding = 'same')(encoded)\n",
        "  \n",
        "  \n",
        "  decoded = Conv1D(8,3,padding = 'same')(encoded)\n",
        "  decoded = Activation('relu')(decoded)\n",
        "  decoded = UpSampling1D(2)(decoded)\n",
        "  \n",
        "  decoded = Conv1D(16,3, padding = 'same')(decoded)\n",
        "  decoded = Activation('relu')(decoded)\n",
        "  decoded = UpSampling1D(2)(decoded)\n",
        "  \n",
        "  decoded = Conv1D(32,3,padding = 'same')(decoded)\n",
        "  decoded = Activation('relu')(decoded)\n",
        "  decoded = UpSampling1D(2)(decoded)\n",
        "  \n",
        "  decoded = Conv1D(1,3,activation = 'sigmoid',padding = 'same')(decoded)\n",
        "  \n",
        "  autoencoder = Model(inputs,decoded)\n",
        "  \n",
        "  optimizer = RMSprop(lr=1e-3)\n",
        "  autoencoder.compile(optimizer = optimizer, loss = 'mse')\n",
        "  \n",
        "  return autoencoder\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def nn_predict(model,prediction_data):\n",
        "  pad_length = timesteps-(np.size(prediction_data)%timesteps)\n",
        "  if pad_length != timesteps:\n",
        "    prediction_data = np.pad(prediction_data,[(0,0),(0,pad_length)],\n",
        "                             mode = 'constant', constant_values = 0)\n",
        "   \n",
        "  prediction_data = prediction_data.reshape((-1,timesteps))\n",
        "  \n",
        "  predicted_data = []\n",
        "  \n",
        "  for i in range(np.size(prediction_data,0)):\n",
        "    pred_part = prediction_data[i].reshape(1,timesteps,features) \n",
        "    pred_part = model.predict(pred_part)\n",
        "    predicted_data.append(pred_part)\n",
        "  \n",
        "  predicted_data = (np.array(predicted_data)).flatten()\n",
        "  \n",
        "  predicted_data = predicted_data.reshape(-1,len(predicted_data))\n",
        "  predicted_data = scaler.inverse_transform(predicted_data)\n",
        "  return predicted_data\n",
        "\n",
        "\n",
        "#HyperParameter Range\n",
        "hyper_parms = {\n",
        "    \"model\":{\n",
        "        \"max_filters\":hp.randint('mx_filters',(32,320))\n",
        "        \"min_filters\":hp.randint('mn_filters',(4,32))\n",
        "        'layers':hp.randint('ly',(3,12))\n",
        "        'filter_spacing':hp.choice('fs'['lin','half','random'])\n",
        "        \n",
        "    }\n",
        "    \"fit\":{\n",
        "        'epochs': 150,\n",
        "        'patience': 3\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "#NN Evaluation\n",
        "db_name = 'sine'\n",
        "exp_name ='exp_1'\n",
        "objective = CompileFN(db_name,exp_name,\n",
        "                      data_fn = create_data,\n",
        "                      model_fm = create_autoencoder,\n",
        "                      loss_metric = 'loss',\n",
        "                      loss_metric_mode = 'min'\n",
        "                      valid_split = None,\n",
        "                      save_model = 'best')\n",
        "#                     save_results = True,\n",
        "#                     save_dir = model_file.SetContentFile('model.h5')\n",
        "                      \n",
        "\n",
        "\n",
        "# model_file.Upload()\n",
        "# drive.CreateFile({'id': model_file.get('id')})\n",
        "\n",
        "# test_fn(objective, hyper_params)\n",
        "# trials = Trials()\n",
        "# best = fmin(objective, hyper_params, trials=trials, algo=tpe.suggest, max_evals=2)\n",
        "# trials = KMongoTrials(db_name, exp_name,\n",
        "#                       ip=\"localhost\",\n",
        "# \t              port=22334)\n",
        "# best = fmin(objective, hyper_params, trials=trials, algo=tpe.suggest, max_evals=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#NN HyperParameters\n",
        "# epochs = 1\n",
        "\n",
        "# inputs = 1\n",
        "\n",
        "# #NN Fitting\n",
        "# autoencoder = create_autoencoder(inputs,filter_spacing = 'lin')\n",
        "# autoencoder_correct = create_Conv1D()\n",
        "# autoencoder_correct.summary()\n",
        "# autoencoder.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Prediction Data\n",
        "#prediction = nn_predict(autoencoder,x_pred).flatten()\n",
        "\n",
        "         \n",
        "# #Plot Prediction                                   \n",
        "# fig = plt.figure()\n",
        "# ax = plt.subplot(111)\n",
        "# ax.plot(t,prediction,label = 'Denoised Signal')\n",
        "# ax.plot(t,clean_signal.flatten(), label = 'Clean Signal')\n",
        "# ax.plot(t,x_pred_pre.flatten(), label = 'Noisy Signal')\n",
        "# plt.xlim(0,0.25)\n",
        "# ax.legend(loc = 3)\n",
        "  \n",
        "  \n",
        "#Save Model to Google Drive\n",
        "# autoencoder.save('model.h5', include_optimizer=False)\n",
        "# model_file = drive.CreateFile({'title' : 'CNN1D_v1.0.h5'})\n",
        "# model_file.SetContentFile('model.h5')\n",
        "# model_file.Upload()\n",
        "# drive.CreateFile({'id': model_file.get('id')})\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         (None, 2000, 1)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_36 (Conv1D)           (None, 2000, 32)          128       \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 2000, 32)          128       \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 2000, 32)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_16 (MaxPooling (None, 1000, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_37 (Conv1D)           (None, 1000, 16)          1552      \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 1000, 16)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_17 (MaxPooling (None, 500, 16)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_38 (Conv1D)           (None, 500, 8)            392       \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 500, 8)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_18 (MaxPooling (None, 250, 8)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_39 (Conv1D)           (None, 250, 8)            200       \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 250, 8)            0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_16 (UpSampling (None, 500, 8)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_40 (Conv1D)           (None, 500, 16)           400       \n",
            "_________________________________________________________________\n",
            "activation_35 (Activation)   (None, 500, 16)           0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_17 (UpSampling (None, 1000, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_41 (Conv1D)           (None, 1000, 32)          1568      \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 1000, 32)          0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_18 (UpSampling (None, 2000, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_42 (Conv1D)           (None, 2000, 1)           97        \n",
            "=================================================================\n",
            "Total params: 4,465\n",
            "Trainable params: 4,401\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         (None, 2000, 1)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_29 (Conv1D)           (None, 2000, 32)          128       \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 2000, 32)          128       \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 2000, 32)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_13 (MaxPooling (None, 1000, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_30 (Conv1D)           (None, 1000, 17)          1649      \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 1000, 17)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_14 (MaxPooling (None, 500, 17)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_31 (Conv1D)           (None, 500, 17)           884       \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 500, 17)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_15 (MaxPooling (None, 250, 17)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_32 (Conv1D)           (None, 250, 17)           884       \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 250, 17)           0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_13 (UpSampling (None, 500, 17)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_33 (Conv1D)           (None, 500, 17)           884       \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 500, 17)           0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_14 (UpSampling (None, 1000, 17)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_34 (Conv1D)           (None, 1000, 32)          1664      \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 1000, 32)          0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_15 (UpSampling (None, 2000, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_35 (Conv1D)           (None, 2000, 1)           97        \n",
            "=================================================================\n",
            "Total params: 6,318\n",
            "Trainable params: 6,254\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
