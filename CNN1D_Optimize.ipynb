{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN1D Optimize.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelXJames/Signal-Denoising-Autoencoder/blob/master/CNN1D_Optimize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ontfmefnFpH9",
        "colab_type": "code",
        "outputId": "55cc9978-9349-4c07-d773-cae603ac1184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1985
        }
      },
      "cell_type": "code",
      "source": [
        "#Additional Packages to Install\n",
        "# !pip install pydrive\n",
        "# !pip install git+https://github.com/hyperopt/hyperopt.git\n",
        "# !pip install kopt\n",
        "# !pip install oauth2client\n",
        "# !pip install tensorboardcolab\n",
        "# !pip install pymongo\n",
        "# !pip install git+https://github.com/rthalley/dnspython.git\n",
        "# !pip install dnspython\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pydrive\n",
        "import pymongo\n",
        "import dns\n",
        "from tensorboardcolab import *\n",
        "from random import randint\n",
        "from keras.layers import Dense,Conv1D,Input\n",
        "from keras.layers import MaxPooling1D,UpSampling1D\n",
        "from keras.layers import Activation,BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.optimizers import RMSprop\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from kopt import CompileFN, KMongoTrials, test_fn\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "from hyperopt.mongoexp import MongoTrials\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#MongoTrials\n",
        "SRV_string  = 'mongodb+srv://jack_house:BF26AJDCnu6F7rb@autoencoderoptimization-08kaz.mongodb.net/test?retryWrites=true'\n",
        "client = pymongo.MongoClient(SRV_string)\n",
        "client.test_database\n",
        "print(client)\n",
        "hostname = 'autoencoderoptimization-shard-00-02-08kaz.mongodb.net'\n",
        "port = 27017\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "#Authenticate/Create PyDrive client \n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "  \n",
        "#tbc=TensorBoardColab()\n",
        "batch_size = 1000\n",
        "validation_split = 0.15\n",
        "timesteps = 2000\n",
        "features = 1\n",
        "global timesteps\n",
        "global fe#atures\n",
        "\n",
        "\n",
        "#Generate Test/Train Data\n",
        "def create_data(timesteps = 2000):\n",
        "  length = 1\n",
        "  dt = 0.0005\n",
        "  noise_factor = 0.35\n",
        "  data_1 = []\n",
        "  data_noisy_1 = []\n",
        "  for i in range(batch_size):\n",
        "      freq = 30*np.random.random() + 1\n",
        "      t = np.arange(0,length,dt)\n",
        "      #signal = np.arange(-0.5*length,0.5*length,dt)\n",
        "      signal = np.sin(2*np.pi*freq*t)\n",
        "      noisy_signal = signal+noise_factor*np.random.rand(len(t))\n",
        "      data_1.append(signal)\n",
        "      data_noisy_1.append(noisy_signal)\n",
        "  data_1 = np.array(data_1)\n",
        "  data_noisy_1 = np.array(data_noisy_1)\n",
        "  \n",
        "  #Scale Data - Need a better scaler than MinMaxScaler\n",
        "  scaler = MinMaxScaler(feature_range = (0,1))\n",
        "  data_1  = np.vstack((data_1,data_noisy_1))\n",
        "  data_1 = scaler.fit_transform(data_1)\n",
        "  signal = data_1[0:batch_size][:]\n",
        "  signal_noisy = data_1[batch_size:2*batch_size][:]\n",
        "  \n",
        "  #Input Shaping\n",
        "  features = 1\n",
        "  timesteps = len(signal[0])\n",
        "  shape = (batch_size,timesteps,features)\n",
        "  #Input shape should equal [samples, timesteps, features]\n",
        "  x = signal.reshape(shape)\n",
        "  y = signal_noisy.reshape(shape)\n",
        "  \n",
        "  split_point = int((1-validation_split)*batch_size)\n",
        "  \n",
        "  x_train = x[0:split_point,:,:]\n",
        "  y_train = y[0:split_point,:,:]\n",
        "  x_test = x[split_point:batch_size,:,:]\n",
        "  y_test = y[split_point:batch_size][:]\n",
        "\n",
        "  return (x_train,y_train,features),(x_test,y_test)\n",
        "\n",
        "\n",
        "\n",
        "#Gnerate Model For Optimization\n",
        "def create_autoencoder(train_data,max_filters = 32, min_filters = 8, layers = 3,\n",
        "                      kernel_size = 3,filter_spacing = 'lin'):\n",
        "  \n",
        "  timesteps = 2000\n",
        "  features = 1\n",
        "  inputs = Input(shape = (timesteps,features),name = 'Input')\n",
        "  encoded = inputs\n",
        "  \n",
        "  #Linear Filter Range\n",
        "  if filter_spacing == 'lin':\n",
        "    filter_range = np.linspace(max_filters,min_filters,layers,\n",
        "                               dtype = 'int64')\n",
        "  #Half Filter Range\n",
        "  if filter_spacing == 'half':\n",
        "    if max_filters%2 != 0:\n",
        "      max_filters+=1\n",
        "    if min_filters%2 != 0:\n",
        "      min_filters+=1\n",
        "    filter_range = [max_filters]\n",
        "    x = max_filters\n",
        "    while x> min_filters:\n",
        "      x = x/2\n",
        "      filter_range.append(x)\n",
        "    if filter_range[-1] < min_filters:\n",
        "      del filter_range[-1]\n",
        "    filter_range = np.asarray(filter_range,dtype = 'int64')\n",
        "  \n",
        "  #Random Filter Range\n",
        "  if filter_spacing == 'random':\n",
        "    filter_range = [randint(min_filters, max_filters) for z in range(layers)]\n",
        "    filter_range.sort(reverse= True)\n",
        "    filter_range = np.asarray(filter_range)\n",
        "    \n",
        "  conv_name = ''\n",
        "  count = 1\n",
        "  for num_filters in filter_range: \n",
        "    conv_name = 'Conv_Layer_No._{0}'.format(count)\n",
        "    encoded = Conv1D(num_filters,kernel_size,padding = 'same',name = conv_name)(encoded)\n",
        "    count+=1\n",
        "    if(num_filters==max_filters):\n",
        "      encoded = BatchNormalization()(encoded)\n",
        "  \n",
        "    encoded = Activation('relu')(encoded)\n",
        "    encoded = MaxPooling1D(2,padding = 'same')(encoded)\n",
        "  \n",
        "  filter_range = np.sort(filter_range)\n",
        "  for num_filters in filter_range: \n",
        "    conv_name = 'Conv_Layer_No._{0}'.format(count)\n",
        "    encoded = Conv1D(num_filters,kernel_size,padding = 'same',name = conv_name)(encoded)\n",
        "    count+=1\n",
        "    encoded = Activation('relu')(encoded)\n",
        "    encoded = UpSampling1D(2)(encoded)\n",
        "    \n",
        "  decoded = Conv1D(1,kernel_size,activation = 'sigmoid',padding = 'same')(encoded)\n",
        "  \n",
        "  autoencoder = Model(inputs,decoded)\n",
        "  autoencoder.summary()\n",
        "  optimizer = RMSprop(lr=1e-3)\n",
        "  autoencoder.compile(optimizer = optimizer, loss = 'mse')\n",
        "  \n",
        "  return autoencoder\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#HyperParameter Range\n",
        "hyper_params = {\n",
        "    \"data\":{\n",
        "        'timesteps': 2000\n",
        "        \n",
        "        \n",
        "    },\n",
        "    \"model\":{\n",
        "        \"max_filters\":hp.choice('mx_filters',np.arange(32,320,dtype = 'int32')),\n",
        "        \"min_filters\":hp.choice('mn_filters',np.arange(4,32,dtype = 'int32')),\n",
        "        'layers':hp.choice('ly',np.arange(1,3,dtype = 'int32')),\n",
        "        'filter_spacing':hp.choice('fs',['lin','half','random'])\n",
        "        \n",
        "    },\n",
        "    \"fit\":{\n",
        "        'epochs': 1,\n",
        "        'patience': 3\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "#NN Evaluation\n",
        "db_name = 'Autoencoder'\n",
        "exp_name ='exp_1'\n",
        "objective = CompileFN(db_name,exp_name,\n",
        "                      data_fn = create_data,\n",
        "                      model_fn = create_autoencoder,\n",
        "                      loss_metric = 'loss',\n",
        "                      loss_metric_mode = 'min',\n",
        "                      valid_split = None,\n",
        "                      save_model = 'best',\n",
        "                      save_results = True)\n",
        "                      \n",
        "                      \n",
        "\n",
        "\n",
        "# model_file.Upload()\n",
        "# drive.CreateFile({'id': model_file.get('id')})\n",
        "\n",
        "#test_fn(objective, hyper_params)\n",
        "trials = Trials()\n",
        "best = fmin(objective, hyper_params, trials=trials, algo=tpe.suggest, max_evals=2)\n",
        "\n",
        "print(trials.results)\n",
        "\n",
        "#Mongo Trials - Cont'd\n",
        "#trials = MongoTrials(SRV_string,exp_key = 'Optimization_1')\n",
        "#trials = MongoTrials('mongo://{0}:{1}/{2}/jobs'.format(hostname,port,db_name), exp_key='exp1')\n",
        "#trials = KMongoTrials(db_name, exp_name,\n",
        "#                      ip=hostname,\n",
        "#                      port=port)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "#Save Model to Google Drive\n",
        "# model_file = drive.CreateFile({'title' : 'CNN1D_v1.0.h5'})\n",
        "# model_file.SetContentFile('model.h5')\n",
        "# model_file.Upload()\n",
        "# drive.CreateFile({'id': model_file.get('id')})\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-12-29 21:48:45,517 [INFO] tpe_transform took 0.022744 seconds\n",
            "2018-12-29 21:48:45,519 [INFO] TPE using 0 trials\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MongoClient(host=['autoencoderoptimization-shard-00-01-08kaz.mongodb.net:27017', 'autoencoderoptimization-shard-00-00-08kaz.mongodb.net:27017', 'autoencoderoptimization-shard-00-02-08kaz.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, authsource='admin', replicaset='AutoEncoderOptimization-shard-0', ssl=True, retrywrites=True)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2018-12-29 21:48:45,531 [INFO] Load data...\n",
            "2018-12-29 21:48:45,975 [INFO] Fit...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Input (InputLayer)           (None, 2000, 1)           0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._1 (Conv1D)    (None, 2000, 73)          292       \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 2000, 73)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_22 (MaxPooling (None, 1000, 73)          0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._2 (Conv1D)    (None, 1000, 73)          16060     \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 1000, 73)          0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_22 (UpSampling (None, 2000, 73)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_9 (Conv1D)            (None, 2000, 1)           220       \n",
            "=================================================================\n",
            "Total params: 16,572\n",
            "Trainable params: 16,572\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "['loss']\n",
            "Train on 850 samples, validate on 150 samples\n",
            "Epoch 1/1\n",
            " - 9s - loss: 0.0350 - val_loss: 0.0077\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2018-12-29 21:49:16,455 [INFO] Evaluate...\n",
            "2018-12-29 21:49:21,990 [INFO] Done!\n",
            "2018-12-29 21:49:22,019 [INFO] tpe_transform took 0.027420 seconds\n",
            "2018-12-29 21:49:22,020 [INFO] TPE using 1/1 trials with best loss 0.007713\n",
            "2018-12-29 21:49:22,030 [INFO] Load data...\n",
            "2018-12-29 21:49:22,737 [INFO] Fit...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Input (InputLayer)           (None, 2000, 1)           0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._1 (Conv1D)    (None, 2000, 46)          184       \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 2000, 46)          184       \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 2000, 46)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_23 (MaxPooling (None, 1000, 46)          0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._2 (Conv1D)    (None, 1000, 23)          3197      \n",
            "_________________________________________________________________\n",
            "activation_46 (Activation)   (None, 1000, 23)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_24 (MaxPooling (None, 500, 23)           0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._3 (Conv1D)    (None, 500, 11)           770       \n",
            "_________________________________________________________________\n",
            "activation_47 (Activation)   (None, 500, 11)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_25 (MaxPooling (None, 250, 11)           0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._4 (Conv1D)    (None, 250, 11)           374       \n",
            "_________________________________________________________________\n",
            "activation_48 (Activation)   (None, 250, 11)           0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_23 (UpSampling (None, 500, 11)           0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._5 (Conv1D)    (None, 500, 23)           782       \n",
            "_________________________________________________________________\n",
            "activation_49 (Activation)   (None, 500, 23)           0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_24 (UpSampling (None, 1000, 23)          0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_No._6 (Conv1D)    (None, 1000, 46)          3220      \n",
            "_________________________________________________________________\n",
            "activation_50 (Activation)   (None, 1000, 46)          0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_25 (UpSampling (None, 2000, 46)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_10 (Conv1D)           (None, 2000, 1)           139       \n",
            "=================================================================\n",
            "Total params: 8,850\n",
            "Trainable params: 8,758\n",
            "Non-trainable params: 92\n",
            "_________________________________________________________________\n",
            "['loss']\n",
            "Train on 850 samples, validate on 150 samples\n",
            "Epoch 1/1\n",
            " - 12s - loss: 0.0163 - val_loss: 0.0032\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2018-12-29 21:50:05,057 [INFO] Evaluate...\n",
            "2018-12-29 21:50:11,347 [INFO] Done!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[{'loss': 0.007713085971772671, 'status': 'ok', 'eval': {'loss': 0.007713085971772671}, 'param': {'data': {'timesteps': 2000}, 'fit': {'epochs': 1, 'patience': 3, 'batch_size': 32, 'early_stop_monitor': 'val_loss'}, 'model': {'filter_spacing': 'random', 'layers': 1, 'max_filters': 79, 'min_filters': 24}}, 'path': {'model': '/root/.kopt/data//Autoencoder/exp_1//train_models/29983aa1-be0a-423a-9eb0-f336e59e0e34.h5', 'results': '/root/.kopt/data//Autoencoder/exp_1//train_models/29983aa1-be0a-423a-9eb0-f336e59e0e34.json'}, 'name': {'data': 'create_data', 'model': 'create_autoencoder', 'optim_metric': 'loss', 'optim_metric_mode': 'loss'}, 'history': {'params': {'batch_size': 32, 'epochs': 1, 'steps': None, 'samples': 850, 'verbose': 2, 'do_validation': True, 'metrics': ['loss', 'val_loss']}, 'loss': {'epoch': [0], 'val_loss': [0.0077130861890812715], 'loss': [0.03496149937677033]}}, 'time': {'start': '2018-12-29 21:48:45.531200', 'end': '2018-12-29 21:49:21.989637', 'duration': {'total': 36.458437, 'dataload': 0.227239, 'training': 36.231198}}}, {'loss': 0.0031829734798520803, 'status': 'ok', 'eval': {'loss': 0.0031829734798520803}, 'param': {'data': {'timesteps': 2000}, 'fit': {'epochs': 1, 'patience': 3, 'batch_size': 32, 'early_stop_monitor': 'val_loss'}, 'model': {'filter_spacing': 'half', 'layers': 2, 'max_filters': 45, 'min_filters': 6}}, 'path': {'model': '/root/.kopt/data//Autoencoder/exp_1//train_models/2a8d83ff-691f-435b-95b6-732c8a22e950.h5', 'results': '/root/.kopt/data//Autoencoder/exp_1//train_models/2a8d83ff-691f-435b-95b6-732c8a22e950.json'}, 'name': {'data': 'create_data', 'model': 'create_autoencoder', 'optim_metric': 'loss', 'optim_metric_mode': 'loss'}, 'history': {'params': {'batch_size': 32, 'epochs': 1, 'steps': None, 'samples': 850, 'verbose': 2, 'do_validation': True, 'metrics': ['loss', 'val_loss']}, 'loss': {'epoch': [0], 'val_loss': [0.0031829733525713284], 'loss': [0.016274009274647516]}}, 'time': {'start': '2018-12-29 21:49:22.029945', 'end': '2018-12-29 21:50:11.347361', 'duration': {'total': 49.317416, 'dataload': 0.222829, 'training': 49.094587}}}]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a91dcc357451>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;31m#Save Model to Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0mmodel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreateFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'title'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'CNN1D_v1.0.h5'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m \u001b[0mmodel_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetContentFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUpload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreateFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydrive/files.py\u001b[0m in \u001b[0;36mSetContentFile\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model.h5'"
          ]
        }
      ]
    }
  ]
}